# Self-DistilT

This repository contains the implementation and research report for **SelfDistil-T: Self-Distilling Transformers via EMA Teachers, Layer-wise Predictive Alignment, Progressive Freezing, and LayerDrop**.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ SelfDistil-T.pdf       # Final project report (PhD-level analysis)
â”œâ”€â”€ config.py              # Configuration file (hyperparameters, paths, settings)
â”œâ”€â”€ data.py                # Dataset utilities (loading, preprocessing)
â”œâ”€â”€ distillation.py        # Core distillation methods (losses, alignment)
â”œâ”€â”€ ema.py                 # EMA teacher implementation
â”œâ”€â”€ example.ipynb          # Example Jupyter notebook (training & evaluation demo)
â”œâ”€â”€ model.py               # Transformer model definition
â”œâ”€â”€ scheduler.py           # Learning rate and freezing schedule logic
â”œâ”€â”€ train.py               # Training script (integrating all components)
â”œâ”€â”€ utils.py               # Helper functions (logging, metrics, etc.)
```

---

## ğŸ“– Contents

- **Report (`SelfDistil-T.pdf`)**  
  Full description of methodology, theoretical analysis, experiments, and results.

- **Source Code**  
  - `model.py`: Defines the causal decoder-only transformer architecture.  
  - `ema.py`: Maintains an exponential moving average (EMA) teacher.  
  - `distillation.py`: Implements LM loss, KD loss, and representation alignment loss.  
  - `scheduler.py`: Progressive freezing and LayerDrop schedule.  
  - `train.py`: Main entry point for training.  
  - `data.py`: Data preprocessing and loaders.  
  - `config.py`: Centralized hyperparameters and experiment settings.  
  - `utils.py`: Utilities for metrics, logging, and checkpointing.  
  - `example.ipynb`: Quick-start example to reproduce experiments.

---

## ğŸ”§ Requirements

- Python 3.9+
- PyTorch 2.0+
- HuggingFace Transformers (optional for tokenizer)
- NumPy, SciPy, Matplotlib
- Jupyter (for running `example.ipynb`)

Install dependencies via:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Training

Run training with:

```bash
python train.py --config config.py
```

---

## ğŸ“Š Report Highlights

- EMA teachers reduce variance and stabilize optimization.  
- BYOL-style predictive alignment avoids collapse.  
- Progressive freezing improves efficiency and curriculum learning.  
- LayerDrop increases pruning robustness.  
- Empirical gains on WikiText-2, WikiText-103, and OpenWebText.

---

## ğŸ“š References

Key references included in the bibliography:
- Hinton et al. (2015) â€” *Distilling the Knowledge in a Neural Network*
- Tarvainen & Valpola (2017) â€” *Mean Teacher*
- Grill et al. (2020) â€” *Bootstrap Your Own Latent (BYOL)*
- Fan et al. (2020) â€” *LayerDrop*
- Sanh et al. (2019) â€” *DistilBERT*
- Furlanello et al. (2018) â€” *Born-Again Neural Networks*

---

## âœ¨ Author

- **Khalil Braham** (France)  
  ğŸ“§ khalilbrahem.kb@gmail.com
